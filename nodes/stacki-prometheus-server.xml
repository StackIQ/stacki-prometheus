<?xml version="1.0" standalone="no"?>

<kickstart>


<description>
Your stacki-prometheus pallet description here
</description>

<si_copyright>
(c) 2006 - 2016 StackIQ Inc.
All rights reserved. stacki(r) v3.1 www.stacki.com
</si_copyright>

<package>prometheus</package>
<package>grafana</package>
<package>stacki-dashboards</package>
<package>stacki-prometheus-commands</package>
<package>foundation-py-requests</package>

<post>
mkdir -p /opt/prometheus/dashboards

for f in `ls /opt/prometheus/share/*.json`; do
	sname=`echo $f | cut -d "/" -f 5-`
	sed "s/\${DS_PROMETHEUS}/&hostname;-prometheus/g" $f &gt; /opt/prometheus/dashboards/$sname
	sed -i "s/DS_PROMETHEUS/&hostname;-prometheus/g" /opt/prometheus/dashboards/$sname
done
</post>

<post> 
<file name="/opt/prometheus/bin/grafana_config" perms="0755">
#!/opt/stack/bin/python
import stack.api as api
import requests
import json

url='http://admin:admin@127.0.0.1:3000/api'

payload = {"name":"&hostname;-prometheus", 
	"type":"prometheus", 
	"url":"http://localhost:9090", 
	"access":"proxy", 
	"isDefault":"true", 
	"database":"grafana.db", 
	"user":"admin", "password":"&Kickstart_PrivateRootPassword;"}

def has_it(pallet):
	p = api.Call('list.pallet')
	return [ True for i in p if i['name'] == pallet ]


# Delete it
def delete_ds(url):
	r = requests.get('%s/datasources' % url)
	for d in r.json():
		dsid = d['id']
		r = requests.delete('%s/%s' % (url,dsid))

def add_ds(url,payload):
	r = requests.post('%s/datasources' % url, payload)
	print r.text

def delete_dash(url):
	r = requests.get('%s/search' % url)
	dids = [ d['uri'] for d in r.json() ]
	for d in dids:
		r = requests.delete('%s/dashboards/%s' % (url,d))
	
def get_ds(url):
	r = requests.get('%s/datasources' % url)
	return r.json()

def get_dash(url):
	r = requests.get('%s/search' % url)
	return r.json()

def add_dash(url,fname):
	headers = {"Content-Type": "application/json;charset=UTF-8"}

	with open(fname, 'rb') as f:
		jfile = json.load(f)
	r = requests.post('%s/dashboards/db' % url, headers, jfile)
	print r
	print r.status_code
	print r.text

print ("Deleting datasources and dashboards.")

ds = get_ds(url)
if ds != []:
	delete_ds(url)
dsb = get_dash(url)
if dsb != []:
	delete_dash(url)

print ("adding &hostname;-prometheus datasource...")
add_ds(url,payload)

print ("adding node_exporter metrics...")
add_dash(url,'/opt/prometheus/dashboards/node-exporter-server-metrics_rev4.json')

try:
    if has_it('stacki-kubernetes')[0] == True:
        print "adding kubernetes prometheus"
        add_dash(url,
        '/opt/prometheus/dashboards/kubernetes-pod-monitoring_rev1.json')
except: pass

try:
    if has_it('stacki-docker')[0] == True:
        print "adding docker monitoring"
        add_dash(url,
            '/opt/prometheus/dashboards/docker-monitoring_rev1.json')
except: pass

</file>
</post>

<post cond="docker.swarm">

<file name="/opt/prometheus/bin/grafana_config" mode="append" perms="0755">
print "adding docker swarm monitoring"
add_dash(url, '/opt/prometheus/dashboards/docker-swarm-container-overview_rev20.json')
add_dash(url, '/opt/prometheus/dashboards/docker-engine-metrics_rev2.json')
</file>

</post>

<post>
<file name="/opt/prometheus/etc/prometheus.main">
# my global config
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: '&hostname;-monitor'

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first.rules"
  # - "second.rules"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
</file>

<file name="/opt/prometheus/etc/prometheus.job">
# The job name is added as a label job=job_name to any timeseries scraped from this config.
scrape_configs:
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:</file>
</post>

<post cond="prometheus.firewall">
/opt/stack/bin/stack add host firewall &hostname; network=all \
	table=filter rulename=PROMETHEUS service="9090" \
	protocol="tcp" action="ACCEPT" chain="INPUT" \
	flags="-m state --state NEW" comment="Prometheus"

/opt/stack/bin/stack add host firewall &hostname; network=all \
	table=filter rulename=GRAFANA service="3000" \
	protocol="tcp" action="ACCEPT" chain="INPUT" \
	flags="-m state --state NEW" comment="Grafana"

systemctl restart iptables
</post>

<post>
systemctl enable prometheus grafana-server node_exporter
systemctl start node_exporter prometheus grafana-server

echo "Breathe..."
sleep 10

echo "Configuring Grafana..."
/opt/prometheus/bin/grafana_config

systemctl restart grafana-server

echo "Syncing prometheus.yml..."
stack sync prometheus
systemctl is-active prometheus
</post>

</kickstart>
